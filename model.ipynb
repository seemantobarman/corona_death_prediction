{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Data Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Importing the dataset__\n",
    "original_data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__inserting random values in age__\n",
    "for _i in range(len(original_data)):\n",
    "    if pd.isnull(original_data.loc[_i, \"age\"]):\n",
    "        original_data.loc[_i, \"age\"] = random.randint(20,85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#__Spliting and removing replacing words__\n",
    "column_to_add = []\n",
    "for _symptom in original_data.loc[:,\"symptoms\"]:\n",
    "   if type(_symptom) == str:\n",
    "      _temp = _symptom.replace(\"feve\",\"fever\").replace(\"feaver\",\"fever\").replace(\"coughing\",\"cough\").replace(\"feverr\",\"fever\").replace(\"difficult in breathing\",\"difficulty breathing\")\n",
    "      column_to_add.extend(_temp.split(\",\"))\n",
    "#__individual symptoms__ \n",
    "new_column = []\n",
    "for _column in column_to_add:\n",
    "   _temp_clm = _column.strip()\n",
    "   new_column.append(_temp_clm)\n",
    "\n",
    "#__removing duplicates__\n",
    "new_column = list(dict.fromkeys(new_column))\n",
    "new_column = new_column[:20]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__adding the columns of symptoms in the main file__\n",
    "for _column in new_column:\n",
    "    original_data[_column] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Adding random items from the new_column in the Symptoms__\n",
    "for _i in range(len(original_data)):\n",
    "    if pd.isnull(original_data.loc[_i, \"symptoms\"]):\n",
    "        _random_int = random.randint(1,10)\n",
    "        original_data.at[_i, \"symptoms\"] = random.sample(new_column,k=_random_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Addind the missing genders randomly__\n",
    "_gender = [\"male\",\"female\"]\n",
    "\n",
    "for _i in range(len(original_data)):\n",
    "    if pd.isnull(original_data.loc[_i, \"gender\"]):\n",
    "        original_data.at[_i, \"gender\"] = random.choice(_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Addind the missing death numbers randomly__\n",
    "for _i in range(len(original_data)):\n",
    "    if pd.isnull(original_data.loc[_i, \"death\"]):\n",
    "        original_data.at[_i, \"death\"] = random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Filling all the symptoms cells__\n",
    "original_data.iloc[:,5:]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#__Selecting 1 if symptoms are detected__\n",
    "for _i in range(len(original_data)):\n",
    "    for _each in original_data.loc[_i, \"symptoms\"]:\n",
    "        original_data.at[_i,_each] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Final processed dataset__\n",
    "processed_data = original_data.drop(labels=\"symptoms\", axis=1).iloc[:3000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Independent variable__\n",
    "X = processed_data.iloc[:,2:24].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Dependent Variable (Death)__\n",
    "Y = processed_data.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Encodeing the gender__\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "_labelEncoder_X_Gender = LabelEncoder()\n",
    "X[:,0] = _labelEncoder_X_Gender.fit_transform(X[:,0])\n",
    "\n",
    "_onehotencoder = OneHotEncoder(categorical_features= [0])\n",
    "X = _onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Avoiding dummy variable trap__\n",
    "\n",
    "X = X[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Spliting the dataset into training set and test set__\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Feature Scaling__\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_parameters = sc.fit(X_train)\n",
    "\n",
    "X_test = sc.transform(X_test)\n",
    "X_train = sc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "#__Making the ANN__\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__initializing the ANN__\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Addind the input layer and the first hidden layer__\n",
    "classifier.add(Dense(output_dim=11, init='uniform', activation='relu', input_dim=22))\n",
    "classifier.add(Dropout(p=.1))\n",
    "\n",
    "#__Adding the second hidden layer__\n",
    "classifier.add(Dense(output_dim=11, init='uniform', activation='relu'))\n",
    "classifier.add(Dropout(p=.1))\n",
    "\n",
    "#__Adding the second hidden layer__\n",
    "classifier.add(Dense(output_dim=11, init='uniform', activation='relu'))\n",
    "classifier.add(Dropout(p=.1))\n",
    "\n",
    "#__Adding the output layer__\n",
    "classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "#__Compiling the ANN__\n",
    "classifier.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[==============================] - 0s - loss: 0.5579 - acc: 0.7200\nEpoch 282/500\n2400/2400 [==============================] - 0s - loss: 0.5623 - acc: 0.7167\nEpoch 283/500\n2400/2400 [==============================] - 0s - loss: 0.5600 - acc: 0.7246\nEpoch 284/500\n2400/2400 [==============================] - 0s - loss: 0.5546 - acc: 0.7200\nEpoch 285/500\n2400/2400 [==============================] - 0s - loss: 0.5570 - acc: 0.7150\nEpoch 286/500\n2400/2400 [==============================] - 0s - loss: 0.5578 - acc: 0.7138\nEpoch 287/500\n2400/2400 [==============================] - 0s - loss: 0.5584 - acc: 0.7179\nEpoch 288/500\n2400/2400 [==============================] - 0s - loss: 0.5516 - acc: 0.7304\nEpoch 289/500\n2400/2400 [==============================] - 0s - loss: 0.5607 - acc: 0.7075\nEpoch 290/500\n2400/2400 [==============================] - 0s - loss: 0.5524 - acc: 0.7192\nEpoch 291/500\n2400/2400 [==============================] - 0s - loss: 0.5607 - acc: 0.7104\nEpoch 292/500\n2400/2400 [==============================] - 0s - loss: 0.5588 - acc: 0.7233\nEpoch 293/500\n2400/2400 [==============================] - 0s - loss: 0.5554 - acc: 0.7171\nEpoch 294/500\n2400/2400 [==============================] - 0s - loss: 0.5532 - acc: 0.7213\nEpoch 295/500\n2400/2400 [==============================] - 0s - loss: 0.5634 - acc: 0.7142\nEpoch 296/500\n2400/2400 [==============================] - 0s - loss: 0.5567 - acc: 0.7275\nEpoch 297/500\n2400/2400 [==============================] - 0s - loss: 0.5639 - acc: 0.7142\nEpoch 298/500\n2400/2400 [==============================] - 0s - loss: 0.5617 - acc: 0.7154\nEpoch 299/500\n2400/2400 [==============================] - 0s - loss: 0.5640 - acc: 0.7179\nEpoch 300/500\n2400/2400 [==============================] - 0s - loss: 0.5537 - acc: 0.7213\nEpoch 301/500\n2400/2400 [==============================] - 0s - loss: 0.5504 - acc: 0.7263\nEpoch 302/500\n2400/2400 [==============================] - 0s - loss: 0.5595 - acc: 0.7204\nEpoch 303/500\n2400/2400 [==============================] - 0s - loss: 0.5548 - acc: 0.7225\nEpoch 304/500\n2400/2400 [==============================] - 0s - loss: 0.5579 - acc: 0.7117\nEpoch 305/500\n2400/2400 [==============================] - 0s - loss: 0.5628 - acc: 0.7142\nEpoch 306/500\n2400/2400 [==============================] - 0s - loss: 0.5550 - acc: 0.7217\nEpoch 307/500\n2400/2400 [==============================] - 0s - loss: 0.5623 - acc: 0.7158\nEpoch 308/500\n2400/2400 [==============================] - 0s - loss: 0.5545 - acc: 0.7113\nEpoch 309/500\n2400/2400 [==============================] - 0s - loss: 0.5571 - acc: 0.7313\nEpoch 310/500\n2400/2400 [==============================] - 0s - loss: 0.5571 - acc: 0.7158\nEpoch 311/500\n2400/2400 [==============================] - 0s - loss: 0.5569 - acc: 0.7133\nEpoch 312/500\n2400/2400 [==============================] - 0s - loss: 0.5635 - acc: 0.7150\nEpoch 313/500\n2400/2400 [==============================] - 0s - loss: 0.5579 - acc: 0.7138\nEpoch 314/500\n2400/2400 [==============================] - 0s - loss: 0.5609 - acc: 0.7129\nEpoch 315/500\n2400/2400 [==============================] - 0s - loss: 0.5538 - acc: 0.7200\nEpoch 316/500\n2400/2400 [==============================] - 0s - loss: 0.5566 - acc: 0.7163\nEpoch 317/500\n2400/2400 [==============================] - 0s - loss: 0.5610 - acc: 0.7133\nEpoch 318/500\n2400/2400 [==============================] - 0s - loss: 0.5593 - acc: 0.7242\nEpoch 319/500\n2400/2400 [==============================] - 0s - loss: 0.5553 - acc: 0.7183\nEpoch 320/500\n2400/2400 [==============================] - 0s - loss: 0.5508 - acc: 0.7213\nEpoch 321/500\n2400/2400 [==============================] - 0s - loss: 0.5546 - acc: 0.7125\nEpoch 322/500\n2400/2400 [==============================] - 0s - loss: 0.5567 - acc: 0.7238\nEpoch 323/500\n2400/2400 [==============================] - 0s - loss: 0.5615 - acc: 0.7129\nEpoch 324/500\n2400/2400 [==============================] - 0s - loss: 0.5577 - acc: 0.7121\nEpoch 325/500\n2400/2400 [==============================] - 0s - loss: 0.5608 - acc: 0.7188\nEpoch 326/500\n2400/2400 [==============================] - 0s - loss: 0.5594 - acc: 0.7204\nEpoch 327/500\n2400/2400 [==============================] - 0s - loss: 0.5554 - acc: 0.7192\nEpoch 328/500\n2400/2400 [==============================] - 0s - loss: 0.5637 - acc: 0.7113\nEpoch 329/500\n2400/2400 [==============================] - 0s - loss: 0.5581 - acc: 0.7163\nEpoch 330/500\n2400/2400 [==============================] - 0s - loss: 0.5628 - acc: 0.7029\nEpoch 331/500\n2400/2400 [==============================] - 0s - loss: 0.5654 - acc: 0.7063\nEpoch 332/500\n2400/2400 [==============================] - 0s - loss: 0.5638 - acc: 0.7192\nEpoch 333/500\n2400/2400 [==============================] - 0s - loss: 0.5560 - acc: 0.7196\nEpoch 334/500\n2400/2400 [==============================] - 0s - loss: 0.5584 - acc: 0.7192\nEpoch 335/500\n2400/2400 [==============================] - 0s - loss: 0.5563 - acc: 0.7288\nEpoch 336/500\n2400/2400 [==============================] - 0s - loss: 0.5580 - acc: 0.7150\nEpoch 337/500\n2400/2400 [==============================] - 0s - loss: 0.5648 - acc: 0.7175\nEpoch 338/500\n2400/2400 [==============================] - 0s - loss: 0.5608 - acc: 0.7142\nEpoch 339/500\n2400/2400 [==============================] - 0s - loss: 0.5569 - acc: 0.7258\nEpoch 340/500\n2400/2400 [==============================] - 0s - loss: 0.5554 - acc: 0.7125\nEpoch 341/500\n2400/2400 [==============================] - 0s - loss: 0.5498 - acc: 0.7258\nEpoch 342/500\n2400/2400 [==============================] - 0s - loss: 0.5574 - acc: 0.7208\nEpoch 343/500\n2400/2400 [==============================] - 0s - loss: 0.5577 - acc: 0.7217\nEpoch 344/500\n2400/2400 [==============================] - 0s - loss: 0.5548 - acc: 0.7179\nEpoch 345/500\n2400/2400 [==============================] - 0s - loss: 0.5485 - acc: 0.7229\nEpoch 346/500\n2400/2400 [==============================] - 0s - loss: 0.5554 - acc: 0.7238\nEpoch 347/500\n2400/2400 [==============================] - 0s - loss: 0.5529 - acc: 0.7263\nEpoch 348/500\n2400/2400 [==============================] - 0s - loss: 0.5587 - acc: 0.7204\nEpoch 349/500\n2400/2400 [==============================] - 0s - loss: 0.5633 - acc: 0.7117\nEpoch 350/500\n2400/2400 [==============================] - 0s - loss: 0.5532 - acc: 0.7183\nEpoch 351/500\n2400/2400 [==============================] - 0s - loss: 0.5603 - acc: 0.7129\nEpoch 352/500\n2400/2400 [==============================] - 0s - loss: 0.5573 - acc: 0.7238\nEpoch 353/500\n2400/2400 [==============================] - 0s - loss: 0.5598 - acc: 0.7100\nEpoch 354/500\n2400/2400 [==============================] - 0s - loss: 0.5530 - acc: 0.7125\nEpoch 355/500\n2400/2400 [==============================] - 0s - loss: 0.5555 - acc: 0.7167\nEpoch 356/500\n2400/2400 [==============================] - 0s - loss: 0.5610 - acc: 0.7138\nEpoch 357/500\n2400/2400 [==============================] - 0s - loss: 0.5568 - acc: 0.7196\nEpoch 358/500\n2400/2400 [==============================] - 0s - loss: 0.5572 - acc: 0.7179\nEpoch 359/500\n2400/2400 [==============================] - 0s - loss: 0.5569 - acc: 0.7133\nEpoch 360/500\n2400/2400 [==============================] - 0s - loss: 0.5571 - acc: 0.7100\nEpoch 361/500\n2400/2400 [==============================] - 0s - loss: 0.5581 - acc: 0.7221\nEpoch 362/500\n2400/2400 [==============================] - 0s - loss: 0.5550 - acc: 0.7129\nEpoch 363/500\n2400/2400 [==============================] - 0s - loss: 0.5568 - acc: 0.7171\nEpoch 364/500\n2400/2400 [==============================] - 0s - loss: 0.5533 - acc: 0.7175\nEpoch 365/500\n2400/2400 [==============================] - 0s - loss: 0.5619 - acc: 0.7133\nEpoch 366/500\n2400/2400 [==============================] - 0s - loss: 0.5566 - acc: 0.7183\nEpoch 367/500\n2400/2400 [==============================] - 0s - loss: 0.5597 - acc: 0.7183\nEpoch 368/500\n2400/2400 [==============================] - 0s - loss: 0.5640 - acc: 0.7033\nEpoch 369/500\n2400/2400 [==============================] - 0s - loss: 0.5533 - acc: 0.7167\nEpoch 370/500\n2400/2400 [==============================] - 0s - loss: 0.5530 - acc: 0.7179\nEpoch 371/500\n2400/2400 [==============================] - 0s - loss: 0.5565 - acc: 0.7125\nEpoch 372/500\n2400/2400 [==============================] - 0s - loss: 0.5586 - acc: 0.7188\nEpoch 373/500\n2400/2400 [==============================] - 0s - loss: 0.5530 - acc: 0.7196\nEpoch 374/500\n2400/2400 [==============================] - 0s - loss: 0.5622 - acc: 0.7150\nEpoch 375/500\n2400/2400 [==============================] - 0s - loss: 0.5517 - acc: 0.7275\nEpoch 376/500\n2400/2400 [==============================] - 0s - loss: 0.5544 - acc: 0.7179\nEpoch 377/500\n2400/2400 [==============================] - 0s - loss: 0.5552 - acc: 0.7158\nEpoch 378/500\n2400/2400 [==============================] - 0s - loss: 0.5605 - acc: 0.7108\nEpoch 379/500\n2400/2400 [==============================] - 0s - loss: 0.5572 - acc: 0.7225\nEpoch 380/500\n2400/2400 [==============================] - 0s - loss: 0.5606 - acc: 0.7100\nEpoch 381/500\n2400/2400 [==============================] - 0s - loss: 0.5561 - acc: 0.7208\nEpoch 382/500\n2400/2400 [==============================] - 0s - loss: 0.5581 - acc: 0.7217\nEpoch 383/500\n2400/2400 [==============================] - 0s - loss: 0.5549 - acc: 0.7204\nEpoch 384/500\n2400/2400 [==============================] - 0s - loss: 0.5527 - acc: 0.7188\nEpoch 385/500\n2400/2400 [==============================] - 0s - loss: 0.5573 - acc: 0.7188\nEpoch 386/500\n2400/2400 [==============================] - 0s - loss: 0.5566 - acc: 0.7121\nEpoch 387/500\n2400/2400 [==============================] - 0s - loss: 0.5598 - acc: 0.7150\nEpoch 388/500\n2400/2400 [==============================] - 0s - loss: 0.5592 - acc: 0.7150\nEpoch 389/500\n2400/2400 [==============================] - 0s - loss: 0.5579 - acc: 0.7142\nEpoch 390/500\n2400/2400 [==============================] - 0s - loss: 0.5580 - acc: 0.7217\nEpoch 391/500\n2400/2400 [==============================] - 0s - loss: 0.5566 - acc: 0.7113\nEpoch 392/500\n2400/2400 [==============================] - 0s - loss: 0.5533 - acc: 0.7213\nEpoch 393/500\n2400/2400 [==============================] - 0s - loss: 0.5600 - acc: 0.7104\nEpoch 394/500\n2400/2400 [==============================] - 0s - loss: 0.5520 - acc: 0.7213\nEpoch 395/500\n2400/2400 [==============================] - 0s - loss: 0.5560 - acc: 0.7204\nEpoch 396/500\n2400/2400 [==============================] - 0s - loss: 0.5623 - acc: 0.7171\nEpoch 397/500\n2400/2400 [==============================] - 0s - loss: 0.5587 - acc: 0.7138\nEpoch 398/500\n2400/2400 [==============================] - 0s - loss: 0.5585 - acc: 0.7138\nEpoch 399/500\n2400/2400 [==============================] - 0s - loss: 0.5537 - acc: 0.7200\nEpoch 400/500\n2400/2400 [==============================] - 0s - loss: 0.5585 - acc: 0.7179\nEpoch 401/500\n2400/2400 [==============================] - 0s - loss: 0.5621 - acc: 0.7096\nEpoch 402/500\n2400/2400 [==============================] - 0s - loss: 0.5504 - acc: 0.7267\nEpoch 403/500\n2400/2400 [==============================] - 0s - loss: 0.5548 - acc: 0.7133\nEpoch 404/500\n2400/2400 [==============================] - 0s - loss: 0.5521 - acc: 0.7188\nEpoch 405/500\n2400/2400 [==============================] - 0s - loss: 0.5598 - acc: 0.7229\nEpoch 406/500\n2400/2400 [==============================] - 0s - loss: 0.5555 - acc: 0.7100\nEpoch 407/500\n2400/2400 [==============================] - 0s - loss: 0.5590 - acc: 0.7104\nEpoch 408/500\n2400/2400 [==============================] - 0s - loss: 0.5543 - acc: 0.7225\nEpoch 409/500\n2400/2400 [==============================] - 0s - loss: 0.5591 - acc: 0.7154\nEpoch 410/500\n2400/2400 [==============================] - 0s - loss: 0.5533 - acc: 0.7171\nEpoch 411/500\n2400/2400 [==============================] - 0s - loss: 0.5567 - acc: 0.7171\nEpoch 412/500\n2400/2400 [==============================] - 0s - loss: 0.5594 - acc: 0.7129\nEpoch 413/500\n2400/2400 [==============================] - 0s - loss: 0.5573 - acc: 0.7158\nEpoch 414/500\n2400/2400 [==============================] - 0s - loss: 0.5481 - acc: 0.7204\nEpoch 415/500\n2400/2400 [==============================] - 0s - loss: 0.5623 - acc: 0.7133\nEpoch 416/500\n2400/2400 [==============================] - 0s - loss: 0.5583 - acc: 0.7029\nEpoch 417/500\n2400/2400 [==============================] - 0s - loss: 0.5534 - acc: 0.7158\nEpoch 418/500\n2400/2400 [==============================] - 0s - loss: 0.5538 - acc: 0.7129\nEpoch 419/500\n2400/2400 [==============================] - 0s - loss: 0.5542 - acc: 0.7129\nEpoch 420/500\n2400/2400 [==============================] - 0s - loss: 0.5554 - acc: 0.7121\nEpoch 421/500\n2400/2400 [==============================] - 0s - loss: 0.5508 - acc: 0.7242\nEpoch 422/500\n2400/2400 [==============================] - 0s - loss: 0.5569 - acc: 0.7221\nEpoch 423/500\n2400/2400 [==============================] - 0s - loss: 0.5608 - acc: 0.7129\nEpoch 424/500\n2400/2400 [==============================] - 0s - loss: 0.5577 - acc: 0.7117\nEpoch 425/500\n2400/2400 [==============================] - 0s - loss: 0.5594 - acc: 0.7171\nEpoch 426/500\n2400/2400 [==============================] - 0s - loss: 0.5574 - acc: 0.7158\nEpoch 427/500\n2400/2400 [==============================] - 0s - loss: 0.5604 - acc: 0.7058\nEpoch 428/500\n2400/2400 [==============================] - 0s - loss: 0.5647 - acc: 0.7096\nEpoch 429/500\n2400/2400 [==============================] - 0s - loss: 0.5635 - acc: 0.7146\nEpoch 430/500\n2400/2400 [==============================] - 0s - loss: 0.5541 - acc: 0.7225\nEpoch 431/500\n2400/2400 [==============================] - 0s - loss: 0.5538 - acc: 0.7158\nEpoch 432/500\n2400/2400 [==============================] - 0s - loss: 0.5566 - acc: 0.7158\nEpoch 433/500\n2400/2400 [==============================] - 0s - loss: 0.5561 - acc: 0.7133\nEpoch 434/500\n2400/2400 [==============================] - 0s - loss: 0.5559 - acc: 0.7138\nEpoch 435/500\n2400/2400 [==============================] - 0s - loss: 0.5605 - acc: 0.7175\nEpoch 436/500\n2400/2400 [==============================] - 0s - loss: 0.5579 - acc: 0.7146\nEpoch 437/500\n2400/2400 [==============================] - 0s - loss: 0.5523 - acc: 0.7242\nEpoch 438/500\n2400/2400 [==============================] - 0s - loss: 0.5491 - acc: 0.7275\nEpoch 439/500\n2400/2400 [==============================] - 0s - loss: 0.5504 - acc: 0.7258\nEpoch 440/500\n2400/2400 [==============================] - 0s - loss: 0.5507 - acc: 0.7246\nEpoch 441/500\n2400/2400 [==============================] - 0s - loss: 0.5586 - acc: 0.7233\nEpoch 442/500\n2400/2400 [==============================] - 0s - loss: 0.5649 - acc: 0.7075\nEpoch 443/500\n2400/2400 [==============================] - 0s - loss: 0.5520 - acc: 0.7104\nEpoch 444/500\n2400/2400 [==============================] - 0s - loss: 0.5591 - acc: 0.7150\nEpoch 445/500\n2400/2400 [==============================] - 0s - loss: 0.5521 - acc: 0.7221\nEpoch 446/500\n2400/2400 [==============================] - 0s - loss: 0.5577 - acc: 0.7171\nEpoch 447/500\n2400/2400 [==============================] - 0s - loss: 0.5511 - acc: 0.7238\nEpoch 448/500\n2400/2400 [==============================] - 0s - loss: 0.5547 - acc: 0.7221\nEpoch 449/500\n2400/2400 [==============================] - 0s - loss: 0.5596 - acc: 0.7154\nEpoch 450/500\n2400/2400 [==============================] - 0s - loss: 0.5486 - acc: 0.7304\nEpoch 451/500\n2400/2400 [==============================] - 0s - loss: 0.5548 - acc: 0.7171\nEpoch 452/500\n2400/2400 [==============================] - 0s - loss: 0.5604 - acc: 0.7092\nEpoch 453/500\n2400/2400 [==============================] - 0s - loss: 0.5584 - acc: 0.7154\nEpoch 454/500\n2400/2400 [==============================] - 0s - loss: 0.5561 - acc: 0.7183\nEpoch 455/500\n2400/2400 [==============================] - 0s - loss: 0.5513 - acc: 0.7196\nEpoch 456/500\n2400/2400 [==============================] - 0s - loss: 0.5585 - acc: 0.7258\nEpoch 457/500\n2400/2400 [==============================] - 0s - loss: 0.5519 - acc: 0.7163\nEpoch 458/500\n2400/2400 [==============================] - 0s - loss: 0.5602 - acc: 0.7088\nEpoch 459/500\n2400/2400 [==============================] - 0s - loss: 0.5535 - acc: 0.7167\nEpoch 460/500\n2400/2400 [==============================] - 0s - loss: 0.5555 - acc: 0.7046\nEpoch 461/500\n2400/2400 [==============================] - 0s - loss: 0.5559 - acc: 0.7108\nEpoch 462/500\n2400/2400 [==============================] - 0s - loss: 0.5553 - acc: 0.7163\nEpoch 463/500\n2400/2400 [==============================] - 0s - loss: 0.5576 - acc: 0.7083\nEpoch 464/500\n2400/2400 [==============================] - 0s - loss: 0.5532 - acc: 0.7188\nEpoch 465/500\n2400/2400 [==============================] - 0s - loss: 0.5512 - acc: 0.7238\nEpoch 466/500\n2400/2400 [==============================] - 0s - loss: 0.5557 - acc: 0.7113\nEpoch 467/500\n2400/2400 [==============================] - 0s - loss: 0.5538 - acc: 0.7196\nEpoch 468/500\n2400/2400 [==============================] - 0s - loss: 0.5585 - acc: 0.7129\nEpoch 469/500\n2400/2400 [==============================] - 0s - loss: 0.5513 - acc: 0.7254\nEpoch 470/500\n2400/2400 [==============================] - 0s - loss: 0.5592 - acc: 0.7129\nEpoch 471/500\n2400/2400 [==============================] - 0s - loss: 0.5608 - acc: 0.7171\nEpoch 472/500\n2400/2400 [==============================] - 0s - loss: 0.5583 - acc: 0.7142\nEpoch 473/500\n2400/2400 [==============================] - 0s - loss: 0.5565 - acc: 0.7150\nEpoch 474/500\n2400/2400 [==============================] - 0s - loss: 0.5529 - acc: 0.7163\nEpoch 475/500\n2400/2400 [==============================] - 0s - loss: 0.5546 - acc: 0.7175\nEpoch 476/500\n2400/2400 [==============================] - 0s - loss: 0.5553 - acc: 0.7192\nEpoch 477/500\n2400/2400 [==============================] - 0s - loss: 0.5618 - acc: 0.7163\nEpoch 478/500\n2400/2400 [==============================] - 0s - loss: 0.5532 - acc: 0.7158\nEpoch 479/500\n2400/2400 [==============================] - 0s - loss: 0.5511 - acc: 0.7225\nEpoch 480/500\n2400/2400 [==============================] - 0s - loss: 0.5499 - acc: 0.7233\nEpoch 481/500\n2400/2400 [==============================] - 0s - loss: 0.5595 - acc: 0.7158\nEpoch 482/500\n2400/2400 [==============================] - 0s - loss: 0.5599 - acc: 0.7125\nEpoch 483/500\n2400/2400 [==============================] - 0s - loss: 0.5670 - acc: 0.7121\nEpoch 484/500\n2400/2400 [==============================] - 0s - loss: 0.5528 - acc: 0.7204\nEpoch 485/500\n2400/2400 [==============================] - 0s - loss: 0.5586 - acc: 0.7100\nEpoch 486/500\n2400/2400 [==============================] - 0s - loss: 0.5580 - acc: 0.7204\nEpoch 487/500\n2400/2400 [==============================] - 0s - loss: 0.5621 - acc: 0.7067\nEpoch 488/500\n2400/2400 [==============================] - 0s - loss: 0.5637 - acc: 0.7058\nEpoch 489/500\n2400/2400 [==============================] - 0s - loss: 0.5522 - acc: 0.7304\nEpoch 490/500\n2400/2400 [==============================] - 0s - loss: 0.5552 - acc: 0.7213\nEpoch 491/500\n2400/2400 [==============================] - 0s - loss: 0.5555 - acc: 0.7108\nEpoch 492/500\n2400/2400 [==============================] - 0s - loss: 0.5551 - acc: 0.7179\nEpoch 493/500\n2400/2400 [==============================] - 0s - loss: 0.5544 - acc: 0.7192\nEpoch 494/500\n2400/2400 [==============================] - 0s - loss: 0.5571 - acc: 0.7150\nEpoch 495/500\n2400/2400 [==============================] - 0s - loss: 0.5600 - acc: 0.7229\nEpoch 496/500\n2400/2400 [==============================] - 0s - loss: 0.5574 - acc: 0.7150\nEpoch 497/500\n2400/2400 [==============================] - 0s - loss: 0.5557 - acc: 0.7167\nEpoch 498/500\n2400/2400 [==============================] - 0s - loss: 0.5536 - acc: 0.7217\nEpoch 499/500\n2400/2400 [==============================] - 0s - loss: 0.5590 - acc: 0.7183\nEpoch 500/500\n2400/2400 [==============================] - 0s - loss: 0.5500 - acc: 0.7246\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x25b20779ac8>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#__Fitting the ANN to the training set__\n",
    "classifier.fit(X_train, Y_train, batch_size=25, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___Part 3 Making the predictions and evaluating the model___\n",
    "\n",
    "#__Predicting the test set results__\n",
    "y_predicted_value = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_boolean = (y_predicted_value>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__making the error matrix also known as the confusion matrix__\n",
    "from sklearn.metrics import confusion_matrix\n",
    "error = confusion_matrix(Y_test,y_predicted_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6133333333333333\n"
    }
   ],
   "source": [
    "accuracy_of_model = (338+30)/600\n",
    "print(accuracy_of_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 11)                253       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 11)                132       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 11)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 11)                132       \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 11)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 12        \n=================================================================\nTotal params: 529\nTrainable params: 529\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([[-0.16492262, -0.22160235, -0.29264566, -0.11867134, -0.0586256 ,\n         -0.09325363,  0.11048749,  0.20164409,  0.17332043,  0.43915755,\n          0.04901023],\n        [ 0.18196039, -0.01918927, -0.13985917, -0.25860259,  0.25009513,\n          0.21607095,  0.00556027,  0.03064279, -0.15968935, -0.13489215,\n         -0.01471647],\n        [ 0.19535112, -0.03196124, -0.04367076, -0.19908369,  0.20959435,\n         -0.0227866 ,  0.27566844,  0.08162256, -0.09238279, -0.43745032,\n         -0.00388048],\n        [ 0.05613401,  0.1843071 , -0.21527131,  0.16150396, -0.1294132 ,\n         -0.28437832,  0.13145058,  0.13125512,  0.00693205, -0.07657239,\n         -0.24640676],\n        [-0.08350907,  0.12244949,  0.02558586,  0.09906688,  0.23752499,\n          0.07119151, -0.0284396 , -0.12484616,  0.15831983, -0.02374751,\n         -0.12400229],\n        [-0.15465376,  0.23454943,  0.33588091,  0.09065024, -0.06194779,\n         -0.01872995,  0.34220913, -0.22944814, -0.02748034,  0.04013478,\n         -0.26611534],\n        [-0.20415035,  0.35033596, -0.21365391,  0.24238041,  0.13441108,\n          0.15734616, -0.11134943, -0.05317457, -0.33942246,  0.12674563,\n         -0.04371062],\n        [ 0.12765123, -0.17720859,  0.11125429,  0.0335146 ,  0.30808821,\n         -0.42897981,  0.11751141, -0.42055601, -0.35681993,  0.10038974,\n         -0.0377268 ],\n        [ 0.08677135,  0.27299988, -0.08219689,  0.04626658, -0.23002337,\n          0.05673566, -0.23602077, -0.17970081, -0.11149421,  0.17822124,\n         -0.12466172],\n        [-0.05412575, -0.18676154, -0.04978416, -0.6637795 ,  0.35662702,\n         -0.28562883, -0.13350354,  0.16523829,  0.10225254,  0.02322493,\n          0.14544448],\n        [-0.14863957, -0.03770337,  0.0908296 ,  0.08134132,  0.35695827,\n          0.10667459,  0.05858509, -0.01617283,  0.05089207, -0.72310883,\n         -0.22137849],\n        [-0.0842054 , -0.15618399, -0.32380518,  0.18693881, -0.07239593,\n          0.03944789, -0.03968615,  0.30476525, -0.01110587,  0.10708147,\n         -0.2843447 ],\n        [-0.00756151, -0.00710788, -0.35353872,  0.09588852,  0.33778787,\n         -0.08878551, -0.23120189, -0.01616361, -0.21446347,  0.2913301 ,\n         -0.0753018 ],\n        [ 0.06633469,  0.24960156, -0.24264534,  0.18265717,  0.24869421,\n         -0.01452252, -0.10155184,  0.00280942,  0.03016585, -0.18088502,\n          0.0445209 ],\n        [-0.17962137,  0.38525385,  0.06217213, -0.72973007,  0.04770967,\n          0.09134836,  0.17227428,  0.18051532, -0.28072789,  0.06327405,\n         -0.1111934 ],\n        [-0.33122087,  0.14185257, -0.03337179,  0.19519487,  0.26040506,\n         -0.25392112,  0.08163171, -0.44196028,  0.40555051, -0.22848086,\n         -0.16211991],\n        [-0.22959469, -0.38933888,  0.02984056,  0.18013556, -0.06462834,\n          0.15500219,  0.26927054,  0.18188757, -0.3577497 ,  0.1323019 ,\n         -0.20602931],\n        [-0.40921798,  0.03683922,  0.13201202,  0.37865588, -0.21603023,\n         -0.23676293,  0.36777771, -0.36470509, -0.00856967, -0.03267225,\n          0.07925535],\n        [-0.16987227,  0.16430181, -0.07044332,  0.07695165,  0.36659893,\n         -0.00704237, -0.06923711, -0.02656984, -0.1324451 , -0.21386908,\n          0.20480852],\n        [-0.13271552,  0.12286069,  0.03915552,  0.23622623,  0.134454  ,\n         -0.2406656 , -0.14235801, -0.17631851,  0.04323481,  0.01578302,\n          0.05132293],\n        [-0.11522496, -0.39715213, -0.02244892,  0.10813121,  0.46190593,\n          0.04627678, -0.09203707, -0.00640274,  0.09130464, -0.0098303 ,\n         -0.09636008],\n        [ 0.13670208, -0.08338876, -0.26798317,  0.01844521,  0.21092294,\n         -0.05222054,  0.43491787, -0.10727609,  0.00742787, -0.09378777,\n         -0.1880511 ]], dtype=float32),\n array([-0.5620724 , -0.5654766 , -0.28067508, -0.73908103, -1.18723583,\n        -0.57885146, -0.9287473 , -0.27993831, -0.25084269, -0.3839435 ,\n        -0.59549707], dtype=float32),\n array([[ 1.57867157,  1.51094651,  1.53964269,  1.57451141,  1.59702659,\n          1.59118295,  1.61734879,  0.69695091,  1.56893098,  1.51220608,\n          1.5054183 ],\n        [ 0.66144282,  0.68121773,  0.68596512,  0.72821182,  0.66964185,\n          0.71375191,  0.72862053,  0.29382461,  0.67305666,  0.69137865,\n          0.6727165 ],\n        [ 0.82960963,  0.8615368 ,  0.82190251,  0.86573738,  0.87555921,\n          0.82383704,  0.83659363,  0.40040305,  0.7914589 ,  0.8351742 ,\n          0.83825719],\n        [ 0.66350466,  0.65232921,  0.67372131,  0.6719889 ,  0.70153433,\n          0.65405256,  0.690485  ,  0.33451062,  0.72990066,  0.67789382,\n          0.65775567],\n        [ 0.63169897,  0.62205333,  0.65782917,  0.63510674,  0.64449078,\n          0.66799849,  0.66431105,  0.24666591,  0.63401955,  0.65876406,\n          0.66088563],\n        [ 1.29794455,  1.17087853,  1.28127599,  1.28772998,  1.31650829,\n          1.29969335,  1.21719658,  0.66889691,  1.24842811,  1.20900941,\n          1.23338437],\n        [ 0.98280096,  0.98062742,  0.98191738,  1.03644574,  1.0979414 ,\n          1.01300907,  1.05819643,  0.49835175,  1.06114101,  0.99380112,\n          0.86819524],\n        [ 0.78762823,  0.77835685,  0.79685557,  0.7834118 ,  0.8110562 ,\n          0.80087388,  0.795434  ,  0.49374741,  0.79369938,  0.78243655,\n          0.71745795],\n        [ 0.91255027,  0.92896372,  0.9408145 ,  0.95294172,  0.98756295,\n          0.98312861,  0.93243432,  0.53950781,  0.95961785,  0.9460547 ,\n          0.83041662],\n        [ 0.76090181,  0.70913577,  0.75469375,  0.68761021,  0.79115337,\n          0.81960922,  0.74696487,  0.42433676,  0.83705914,  0.76011592,\n          0.75295997],\n        [ 1.82594717,  1.84055078,  1.78012609,  1.89263272,  1.76945353,\n          1.80316877,  1.73533869,  0.75613415,  1.83015645,  1.84657073,\n          1.79048121]], dtype=float32),\n array([-0.93837428, -0.96011698, -0.97251481, -0.98639673, -0.94458789,\n        -0.96535039, -0.95855987, -0.24408504, -0.95201236, -0.95260179,\n        -0.90538549], dtype=float32),\n array([[ 0.09861279,  0.01372266, -0.02919618,  0.2060885 ,  0.20586127,\n          0.00720837,  0.22180749,  0.23270191,  0.29403788,  0.21025157,\n          0.21251352],\n        [ 0.16751605, -0.01988818,  0.0135717 ,  0.15026867,  0.20058829,\n         -0.00450344,  0.22421618,  0.15576828,  0.203484  ,  0.23152395,\n          0.16147722],\n        [ 0.19952182,  0.02606143, -0.00645034,  0.16663741,  0.12499148,\n         -0.02865139,  0.22401485,  0.20565856,  0.24472983,  0.12594086,\n          0.24233232],\n        [ 0.16276526, -0.03381439, -0.02501655,  0.12438995,  0.0928155 ,\n         -0.03127417,  0.22600999,  0.17516926,  0.18035297,  0.16553548,\n          0.20225133],\n        [ 0.17544864, -0.04282185, -0.01016856,  0.23375735,  0.22141019,\n         -0.01654852,  0.18412051,  0.15196151,  0.14882787,  0.15069452,\n          0.23708424],\n        [ 0.19311941, -0.0228231 ,  0.00268729,  0.12196305,  0.24750954,\n         -0.01718357,  0.22226311,  0.14613433,  0.2538656 ,  0.21965837,\n          0.24339022],\n        [ 0.14997329, -0.01903559, -0.02710694,  0.19919524,  0.23417857,\n         -0.00383706,  0.27433872,  0.23115391,  0.20289303,  0.18898708,\n          0.2543667 ],\n        [ 0.15509298, -0.02913213, -0.03596751,  0.11324922,  0.1275581 ,\n          0.02911538,  0.13584043,  0.07713831,  0.16377398,  0.11516122,\n          0.08265422],\n        [ 0.19054815, -0.03187401, -0.04566185,  0.20977727,  0.16488874,\n         -0.02966749,  0.16181961,  0.14273995,  0.21332152,  0.17306094,\n          0.18805487],\n        [ 0.19114858,  0.00398656, -0.05511096,  0.27091318,  0.13072899,\n         -0.02634563,  0.21786654,  0.19242793,  0.18760672,  0.16155833,\n          0.20645107],\n        [ 0.1832799 , -0.01474359,  0.01668922,  0.19028521,  0.2431919 ,\n          0.00769798,  0.2664836 ,  0.1457388 ,  0.24709798,  0.19679175,\n          0.14796045]], dtype=float32),\n array([-0.07356447, -0.02351657, -0.03539371, -0.06604734, -0.08499195,\n        -0.03434911, -0.07352237, -0.06666951, -0.07421122, -0.07633133,\n        -0.10026509], dtype=float32),\n array([[-0.09589293],\n        [ 0.00386769],\n        [ 0.00344793],\n        [-0.07384797],\n        [-0.07575004],\n        [-0.00140649],\n        [-0.07994986],\n        [-0.06818008],\n        [-0.08558154],\n        [-0.08759583],\n        [-0.06892217]], dtype=float32),\n array([ 0.23211305], dtype=float32)]"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "classifier.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python35064bitztdvenvvenvc8958502bcc84c59addbfddc77119dad",
   "display_name": "Python 3.5.0 64-bit ('ztd_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}